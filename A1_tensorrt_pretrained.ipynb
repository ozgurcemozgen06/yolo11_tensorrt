{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1755187192483,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "px8lvwY8TVak",
    "outputId": "4e48ca2f-6423-4f8b-c440-9659d6e7d25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 14 15:59:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113266,
     "status": "ok",
     "timestamp": 1755187339319,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "c4Oi3VB9TwWK",
    "outputId": "bb7017f1-1bab-4423-b1fc-01bdae9fe686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install libraries we‚Äôll use\n",
    "!pip install -q ultralytics opencv-python onnx onnxruntime-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbYlH7DIT6Pp"
   },
   "outputs": [],
   "source": [
    "!which trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755187423493,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "WBLY76w5UjE2",
    "outputId": "a1b3e86a-f81b-416d-c768-d6277f1ef02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: trtexec: command not found\n"
     ]
    }
   ],
   "source": [
    "!trtexec --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84886,
     "status": "ok",
     "timestamp": 1755187795273,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "eT_aapYZUqZ5",
    "outputId": "fe148bac-b08e-43d8-e10b-90f2dc3ef099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorrt\n",
      "  Downloading tensorrt-10.13.2.6.tar.gz (40 kB)\n",
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorrt_cu13==10.13.2.6 (from tensorrt)\n",
      "  Downloading tensorrt_cu13-10.13.2.6.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorrt_cu13_libs==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading tensorrt_cu13_libs-10.13.2.6.tar.gz (704 bytes)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorrt_cu13_bindings==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading tensorrt_cu13_bindings-10.13.2.6-cp311-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
      "Collecting nvidia-cuda-runtime-cu13 (from tensorrt_cu13_libs==10.13.2.6->tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl.metadata (225 bytes)\n",
      "Downloading tensorrt_cu13_bindings-10.13.2.6-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl (1.2 kB)\n",
      "Building wheels for collected packages: tensorrt, tensorrt_cu13, tensorrt_cu13_libs\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorrt: filename=tensorrt-10.13.2.6-py2.py3-none-any.whl size=46436 sha256=73db0168556a2f30a1a67b1aeac0c6474ec7e99a4e625026326cf1aebcf2af91\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/c3/4f/b5a62d55edf240d6a6e82bdba1994887a4c861d7a1ed415d0d\n",
      "  Building wheel for tensorrt_cu13 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorrt_cu13: filename=tensorrt_cu13-10.13.2.6-py2.py3-none-any.whl size=17437 sha256=4c0b4c4ed17ed10118b6fd7586cf572de6c09562054aa2893c4a65a000372720\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/74/87/c19c3dec6eb4d121b960b20e01306caea68b42a6bd82279a2e\n",
      "  Building wheel for tensorrt_cu13_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorrt_cu13_libs: filename=tensorrt_cu13_libs-10.13.2.6-py2.py3-none-manylinux_2_28_x86_64.whl size=2740427176 sha256=044468b0705728c0c8f0e95325ba881ff3e23bd2eebce0b1e628953ff90bbb5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/6c/9f/8c/e380b752b4def86473a5c50fa3b4566cdcc7d1df774c97a505\n",
      "Successfully built tensorrt tensorrt_cu13 tensorrt_cu13_libs\n",
      "Installing collected packages: tensorrt_cu13_bindings, nvidia-cuda-runtime-cu13, tensorrt_cu13_libs, tensorrt_cu13, tensorrt\n",
      "Successfully installed nvidia-cuda-runtime-cu13-0.0.0a0 tensorrt-10.13.2.6 tensorrt_cu13-10.13.2.6 tensorrt_cu13_bindings-10.13.2.6 tensorrt_cu13_libs-10.13.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37899,
     "status": "ok",
     "timestamp": 1755252337170,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "wulPAf7oXQdq",
    "outputId": "bc787f7e-f7a8-4e2b-86bf-bcda98f03352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import torch, os, sys, pathlib\n",
    "assert torch.cuda.is_available(), \"Colab GPU is not enabled. Go to Runtime ‚Üí Change runtime type ‚Üí GPU.\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1755252451447,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "5_0YbyH_MG02",
    "outputId": "cb4e13f2-6495-4083-d6e8-0427047b686a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video OK: /content/drive/MyDrive/yolo11_trt_demo/video/sample_video.mp4\n"
     ]
    }
   ],
   "source": [
    "VIDEO_PATH = \"/content/drive/MyDrive/yolo11_trt_demo/video/sample_video.mp4\"\n",
    "\n",
    "from pathlib import Path\n",
    "vp = Path(VIDEO_PATH)\n",
    "assert vp.exists(), f\"Video not found at: {VIDEO_PATH}\\nDouble-check the path under /content/drive/MyDrive/...\"\n",
    "\n",
    "print(\"Video OK:\", VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1755252668901,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "Q_q0yNSOMuP6",
    "outputId": "52ce29ab-1722-4619-cb4f-45eb03c05114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 15 10:11:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, subprocess, os\n",
    "\n",
    "!nvidia-smi\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73971,
     "status": "ok",
     "timestamp": 1755252763737,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "KJ8fvnDnM6RS",
    "outputId": "e5d6caf3-0345-42c9-f5d3-de42b1734fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT not importable yet: No module named 'tensorrt'\n",
      ">>> /usr/bin/python3 -m pip install -U --extra-index-url https://pypi.nvidia.com tensorrt\n",
      "Installed TensorRT: 10.13.2.6\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, importlib\n",
    "\n",
    "def pip(*args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\"] + list(args)\n",
    "    print(\">>>\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# 1) Try the standard metapackage (from NVIDIA‚Äôs index)\n",
    "ok = False\n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    print(\"TensorRT already present:\", trt.__version__)\n",
    "    ok = True\n",
    "except Exception as e:\n",
    "    print(\"TensorRT not importable yet:\", e)\n",
    "\n",
    "if not ok:\n",
    "    try:\n",
    "        pip(\"install\", \"-U\", \"--extra-index-url\", \"https://pypi.nvidia.com\", \"tensorrt\")\n",
    "        import tensorrt as trt\n",
    "        print(\"Installed TensorRT:\", trt.__version__)\n",
    "        ok = True\n",
    "    except Exception as e:\n",
    "        print(\"Standard install failed:\", e)\n",
    "\n",
    "# 2) Fallback: explicitly install the CUDA 12 build (common on Colab)\n",
    "if not ok:\n",
    "    try:\n",
    "        pip(\"install\", \"-U\", \"--extra-index-url\", \"https://pypi.nvidia.com\", \"tensorrt-cu12\")\n",
    "        import tensorrt as trt\n",
    "        print(\"Installed TensorRT (cu12):\", trt.__version__)\n",
    "        ok = True\n",
    "    except Exception as e:\n",
    "        print(\"cu12 install failed:\", e)\n",
    "\n",
    "# 3) Final nudge: force-reinstall (clears any partial/broken state)\n",
    "if not ok:\n",
    "    try:\n",
    "        pip(\"install\", \"-U\", \"--force-reinstall\", \"--extra-index-url\", \"https://pypi.nvidia.com\", \"tensorrt\")\n",
    "        import tensorrt as trt\n",
    "        print(\"Installed TensorRT (force):\", trt.__version__)\n",
    "        ok = True\n",
    "    except Exception as e:\n",
    "        print(\"Force install failed:\", e)\n",
    "\n",
    "assert ok, \"TensorRT pip wheels were not installed. Check Python version compatibility in this Colab runtime.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212274,
     "status": "ok",
     "timestamp": 1755253139327,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "FUN8v24zNoo6",
    "outputId": "b48601a9-fd47-4bba-cd94-7a741e083ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
      "Fri Aug 15 10:15:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      ">>> /usr/bin/python3 -m pip uninstall -y tensorrt tensorrt-cu12\n",
      ">>> /usr/bin/python3 -m pip install -U ultralytics onnx onnxruntime-gpu\n",
      ">>> /usr/bin/python3 -m pip install -U --extra-index-url https://pypi.nvidia.com tensorrt-cu12\n",
      "TensorRT imported OK: 10.13.2.6 (tensorrt-cu12)\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî‚Äî‚Äî Inspect runtime ‚Äî‚Äî‚Äî\n",
    "import sys, platform, os, subprocess, re\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Colab GPU?\n",
    "!nvidia-smi || echo \"No GPU visible. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\"\n",
    "\n",
    "# Quick Python version advice for TensorRT wheels:\n",
    "py_major_minor = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "if py_major_minor not in {\"3.10\", \"3.11\"}:\n",
    "    print(f\"NOTE: You‚Äôre on Python {py_major_minor}. If TensorRT wheels fail to import, \"\n",
    "          \"switch to Python 3.10 or 3.11 in Colab and rerun.\")\n",
    "\n",
    "def pip(*args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\"] + list(args)\n",
    "    print(\">>>\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Clean up any partial/broken installs (optional but helps) ‚Äî‚Äî‚Äî\n",
    "try:\n",
    "    pip(\"uninstall\", \"-y\", \"tensorrt\", \"tensorrt-cu12\")\n",
    "except Exception as e:\n",
    "    print(\"Uninstall note:\", e)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Install core deps ‚Äî‚Äî‚Äî\n",
    "pip(\"install\", \"-U\", \"ultralytics\", \"onnx\", \"onnxruntime-gpu\")\n",
    "\n",
    "# Try TRT cu12 first (Colab GPUs are CUDA 12.x), then fallback to meta\n",
    "trt_ok = False\n",
    "for pkg in ( \"tensorrt-cu12\", \"tensorrt\" ):\n",
    "    try:\n",
    "        pip(\"install\", \"-U\", \"--extra-index-url\", \"https://pypi.nvidia.com\", pkg)\n",
    "        import importlib; importlib.invalidate_caches()\n",
    "        import tensorrt as trt  # test import right after install\n",
    "        print(\"TensorRT imported OK:\", trt.__version__, f\"({pkg})\")\n",
    "        trt_ok = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Install/import failed for {pkg} ‚Üí\", e)\n",
    "\n",
    "if not trt_ok:\n",
    "    raise SystemExit(\"TensorRT wheel install failed. If you‚Äôre on Python 3.12, switch runtime to 3.10/3.11 and rerun.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5826,
     "status": "ok",
     "timestamp": 1755253145182,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "duCMNQRIN-q2",
    "outputId": "4c13d6b1-e63f-40be-9048-cadd1c743dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Ultralytics: 8.3.179\n",
      "TensorRT: 10.13.2.6\n",
      "OpenCV: 4.12.0\n",
      "\n",
      "‚úÖ Environment is ready. Proceed with the next cells (Config ‚Üí Calibration ‚Üí Exports ‚Üí Benchmark).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import ultralytics, onnx, tensorrt as trt\n",
    "    import cv2, numpy as np, time, yaml, glob, shutil, math, random\n",
    "    print(\"Ultralytics:\", ultralytics.__version__)\n",
    "    print(\"TensorRT:\", trt.__version__)\n",
    "    print(\"OpenCV:\", cv2.__version__)\n",
    "except Exception as e:\n",
    "    print(\"Imports still failing ‚Üí\", repr(e))\n",
    "    print(\"\\nFixes:\")\n",
    "    print(\"1) Runtime ‚Üí Restart runtime (one time) and re-run the two cells above.\")\n",
    "    print(\"2) Ensure Colab is set to GPU and Python 3.10/3.11.\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úÖ Environment is ready. Proceed with the next cells (Config ‚Üí Calibration ‚Üí Exports ‚Üí Benchmark).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1755253677000,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "VYTHep29Oqhh"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# Inference & export \n",
    "IMGSZ = 640\n",
    "CONF  = 0.50\n",
    "WARMUP_FRAMES = 20    \n",
    "LIMIT_FRAMES  = None   \n",
    "\n",
    "# Folders\n",
    "OUT_DIR   = Path(\"/content/outs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CALIB_DIR = Path(\"/content/calib/images\"); CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CALIB_YAML = Path(\"/content/calib.yaml\")\n",
    "\n",
    "# Outputs\n",
    "ENG_FP32 = OUT_DIR/\"yolo11m_fp32.engine\"\n",
    "ENG_FP16 = OUT_DIR/\"yolo11m_fp16.engine\"\n",
    "ENG_INT8 = OUT_DIR/\"yolo11m_int8.engine\"\n",
    "\n",
    "# Baseline model\n",
    "BASE_PT = YOLO(\"yolo11m.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427625,
     "status": "ok",
     "timestamp": 1755254529014,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "z-lKPnmYQEtf",
    "outputId": "eccbddcf-17d3-4a13-b0f0-fb3a8c030d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 480 calibration frames ‚Üí /content/calib/images\n",
      "num_calib = 480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os, cv2, random\n",
    "VIDEO_PATH = \"/content/drive/MyDrive/yolo11_trt_demo/video/sample_video.mp4\"  \n",
    "assert Path(VIDEO_PATH).is_file(), f\"Video not found: {VIDEO_PATH}\"\n",
    "\n",
    "# Quick try: can OpenCV open it?\n",
    "_cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not _cap.isOpened():\n",
    "    print(\"OpenCV couldn't open the original file. Trying a transcode to a Colab-friendly MP4...\")\n",
    "    !ffmpeg -y -i \"$VIDEO_PATH\" -c:v libx264 -pix_fmt yuv420p -movflags +faststart /content/_video_fixed.mp4\n",
    "    VIDEO_PATH = \"/content/_video_fixed.mp4\"\n",
    "    print(\"Transcoded ‚Üí\", VIDEO_PATH)\n",
    "_cap.release()\n",
    "\n",
    "def extract_calib_frames_from_videos(video_paths, out_dir, target_total=480, imgsz=640, seed=0):\n",
    "    from pathlib import Path\n",
    "    import cv2, numpy as np\n",
    "\n",
    "   \n",
    "    if isinstance(video_paths, (str, Path)):\n",
    "        video_paths = [str(video_paths)]\n",
    "    else:\n",
    "        video_paths = [str(p) for p in video_paths]\n",
    "\n",
    "    random.seed(seed)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nvid = max(1, len(video_paths))\n",
    "    per_video = max(1, target_total // nvid)\n",
    "    saved = 0\n",
    "\n",
    "    for vp in video_paths:\n",
    "        cap = cv2.VideoCapture(vp)\n",
    "        assert cap.isOpened(), f\"Cannot open video: {vp}\"\n",
    "        n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or per_video*2\n",
    "\n",
    "        \n",
    "        if per_video == 1:\n",
    "            idxs = [n // 2]\n",
    "        else:\n",
    "            idxs = sorted(set(int(i*(n-1)/(per_video-1)) for i in range(per_video)))\n",
    "        jitter = max(1, n // (per_video*8 if per_video>0 else 8))\n",
    "        idxs = [max(0, min(n-1, i + random.randint(-jitter, jitter))) for i in idxs]\n",
    "\n",
    "        for idx in idxs:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                continue\n",
    "            h, w = frame.shape[:2]\n",
    "            side = min(h, w)\n",
    "            y0 = (h - side) // 2; x0 = (w - side) // 2\n",
    "            crop = frame[y0:y0+side, x0:x0+side]\n",
    "            img = cv2.resize(crop, (imgsz, imgsz), interpolation=cv2.INTER_AREA)\n",
    "            cv2.imwrite(str(out_dir / f\"calib_{saved:05d}.jpg\"), img)\n",
    "            saved += 1\n",
    "        cap.release()\n",
    "    print(f\"Saved {saved} calibration frames ‚Üí {out_dir}\")\n",
    "    return saved\n",
    "\n",
    "\n",
    "CALIB_DIR = \"/content/calib/images\"\n",
    "IMGSZ = 640\n",
    "num_calib = extract_calib_frames_from_videos([VIDEO_PATH], CALIB_DIR, target_total=480, imgsz=IMGSZ)\n",
    "print(\"num_calib =\", num_calib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1755254729021,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "UPnZa2tXRxPI",
    "outputId": "51dbbe61-a685-4c27-d6f2-aee8bc904151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration YAML ‚Üí /content/calib.yaml\n",
      "Classes: 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BASE_PT\n",
    "names = getattr(getattr(model, \"model\", None), \"names\", None) or getattr(model, \"names\", None)\n",
    "if isinstance(names, dict):\n",
    "    names = [names[i] for i in range(len(names))]\n",
    "if not isinstance(names, (list, tuple)):\n",
    "    names = [str(i) for i in range(80)] \n",
    "\n",
    "calib_cfg = {\n",
    "    \"path\": \"/content/calib\",\n",
    "    \"train\": \"images\",     \n",
    "    \"val\": \"images\",       # calibrator reads these for INT8 activation ranges\n",
    "    \"nc\": int(len(names)),\n",
    "    \"names\": list(map(str, names))\n",
    "}\n",
    "with open(CALIB_YAML, \"w\") as f:\n",
    "    yaml.safe_dump(calib_cfg, f)\n",
    "\n",
    "print(f\"Calibration YAML ‚Üí {CALIB_YAML}\\nClasses: {len(names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755254832627,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "xBW75m6xVafq"
   },
   "outputs": [],
   "source": [
    "def find_newest_engine(since_time: float):\n",
    "    paths = [p for p in glob.glob(\"**/*.engine\", recursive=True)]\n",
    "    paths = [(p, os.path.getmtime(p)) for p in paths if os.path.getmtime(p) >= since_time - 5]\n",
    "    if not paths:\n",
    "        \n",
    "        paths = [(p, os.path.getmtime(p)) for p in glob.glob(\"**/*.engine\", recursive=True)]\n",
    "    if not paths:\n",
    "        return None\n",
    "    paths.sort(key=lambda x: x[1], reverse=True)\n",
    "    return paths[0][0]\n",
    "\n",
    "def export_engine(dest_path: Path, *, half=False, int8=False, data_yaml=None, imgsz=640, batch=1, workspace=4.0, fraction=None):\n",
    "    t0 = time.time()\n",
    "    args = dict(format=\"engine\", imgsz=imgsz, half=half, int8=int8, batch=batch, workspace=workspace)\n",
    "    if int8:\n",
    "        assert data_yaml is not None and Path(data_yaml).exists(), \"INT8 export needs data=calibration_yaml\"\n",
    "        args.update(dict(data=str(data_yaml)))\n",
    "        if fraction is not None:\n",
    "            args.update(dict(fraction=float(fraction)))\n",
    "    # Do export\n",
    "    _ = model.export(**args)\n",
    "    # Locate the engine file produced\n",
    "    found = find_newest_engine(t0)\n",
    "    assert found, \"Engine file not found after export.\"\n",
    "    shutil.move(found, dest_path)\n",
    "    print((\"FP16\" if half else \"INT8\" if int8 else \"FP32\"), \"engine ‚Üí\", dest_path)\n",
    "    return str(dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1059738,
     "status": "ok",
     "timestamp": 1755255917627,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "yJxbV9btVzzY",
    "outputId": "1defebd0-d3d0-4762-c59a-9247917aa16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.179 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "üí° ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
      "YOLO11m summary (fused): 125 layers, 20,091,712 parameters, 0 gradients, 68.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (38.8 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.59'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 2.9s\n",
      "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.64...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 8.5s, saved as 'yolo11m.onnx' (76.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.2.6...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as yolo11m.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 118.9s, saved as 'yolo11m.engine' (86.6 MB)\n",
      "\n",
      "Export complete (121.7s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11m.engine imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11m.engine imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "FP32 engine ‚Üí /content/outs/yolo11m_fp32.engine\n",
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.179 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,091,712 parameters, 0 gradients, 68.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (38.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.64...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.9s, saved as 'yolo11m.onnx' (76.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.2.6...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo11m.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 336.7s, saved as 'yolo11m.engine' (41.9 MB)\n",
      "\n",
      "Export complete (337.2s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11m.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=yolo11m.engine imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml half \n",
      "Visualize:       https://netron.app\n",
      "FP16 engine ‚Üí /content/outs/yolo11m_fp16.engine\n",
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.179 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,091,712 parameters, 0 gradients, 68.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11m.pt' with input shape (8, 3, 640, 640) BCHW and output shape(s) (8, 84, 8400) (38.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.64...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.8s, saved as 'yolo11m.onnx' (76.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.2.6...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m collecting INT8 calibration images from 'data=/content/calib.yaml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 21.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 94.0¬±134.2 MB/s, size: 80.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning /content/calib/labels... 0 images, 480 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [00:00<00:00, 669.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è No labels found in /content/calib/labels.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New cache created: /content/calib/labels.cache\n",
      "WARNING ‚ö†Ô∏è Labels are missing or empty in /content/calib/labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(8, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(8, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building INT8 engine as yolo11m.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 599.9s, saved as 'yolo11m.engine' (25.0 MB)\n",
      "\n",
      "Export complete (600.7s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11m.engine imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=yolo11m.engine imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "INT8 engine ‚Üí /content/outs/yolo11m_int8.engine\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/outs/yolo11m_int8.engine'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FP32\n",
    "export_engine(ENG_FP32, half=False, int8=False, imgsz=IMGSZ, batch=1, workspace=4.0)\n",
    "\n",
    "# FP16\n",
    "export_engine(ENG_FP16, half=True,  int8=False, imgsz=IMGSZ, batch=1, workspace=4.0)\n",
    "\n",
    "# INT8  (use all calibration images; you can lower 'fraction' to speed up)\n",
    "calib_batch = min(8, max(1, num_calib // 8))\n",
    "export_engine(ENG_INT8, half=False, int8=True, data_yaml=CALIB_YAML, imgsz=IMGSZ, batch=calib_batch, workspace=4.0, fraction=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286474,
     "status": "ok",
     "timestamp": 1755257231903,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "bucDHTwsV5-B",
    "outputId": "069ff7e1-2c6e-4ffa-e2af-c590c435b854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.179 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (38.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.64...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.7s, saved as 'yolo11m.onnx' (76.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.2.6...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m collecting INT8 calibration images from 'data=/content/calib.yaml'\n",
      "Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1171.6¬±504.8 MB/s, size: 79.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning /content/calib/labels.cache... 0 images, 480 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Labels are missing or empty in /content/calib/labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building INT8 engine as yolo11m.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 286.3s, saved as 'yolo11m.engine' (24.7 MB)\n",
      "\n",
      "Export complete (286.4s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11m.engine imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=yolo11m.engine imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "INT8 engine ‚Üí /content/outs/yolo11m_int8.engine\n"
     ]
    }
   ],
   "source": [
    "# Re-export INT8 with batch=1 so it matches single-frame inference\n",
    "from pathlib import Path\n",
    "import os, time, shutil, glob\n",
    "\n",
    "def find_newest_engine(since_time: float):\n",
    "    paths = [p for p in glob.glob(\"**/*.engine\", recursive=True)]\n",
    "    paths = [(p, os.path.getmtime(p)) for p in paths if os.path.getmtime(p) >= since_time - 5]\n",
    "    if not paths:\n",
    "        paths = [(p, os.path.getmtime(p)) for p in glob.glob(\"**/*.engine\", recursive=True)]\n",
    "    if not paths:\n",
    "        return None\n",
    "    paths.sort(key=lambda x: x[1], reverse=True)\n",
    "    return paths[0][0]\n",
    "\n",
    "def export_engine(dest_path, *, half=False, int8=False, data_yaml=None, imgsz=640, batch=1, workspace=4.0, fraction=None, dynamic=False):\n",
    "    t0 = time.time()\n",
    "    args = dict(format=\"engine\", imgsz=imgsz, half=half, int8=int8, batch=batch, workspace=workspace, dynamic=dynamic)\n",
    "    if int8:\n",
    "        assert data_yaml is not None and Path(data_yaml).exists(), \"INT8 export needs data=calibration_yaml\"\n",
    "        args.update(dict(data=str(data_yaml)))\n",
    "        if fraction is not None:\n",
    "            args.update(dict(fraction=float(fraction)))\n",
    "    _ = model.export(**args)\n",
    "    found = find_newest_engine(t0)\n",
    "    assert found, \"Engine file not found after export.\"\n",
    "    shutil.move(found, dest_path)\n",
    "    print((\"FP16\" if half else \"INT8\" if int8 else \"FP32\"), \"engine ‚Üí\", dest_path)\n",
    "\n",
    "# Remove old INT8 engine (batch=8) if present\n",
    "try:\n",
    "    Path(ENG_INT8).unlink()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Rebuild INT8 with batch=1 (static). Keep fraction=1.0; you can use 0.25 for a quick test build.\n",
    "export_engine(ENG_INT8, half=False, int8=True, data_yaml=CALIB_YAML,\n",
    "              imgsz=IMGSZ, batch=1, workspace=4.0, fraction=1.0, dynamic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1755257433826,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "1w9efqGWd3np"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2, time, numpy as np\n",
    "\n",
    "def benchmark_and_write(model_path, tag, out_mp4, imgsz=IMGSZ, conf=CONF,\n",
    "                        warmup=WARMUP_FRAMES, limit=LIMIT_FRAMES, device=0):\n",
    "    is_engine = str(model_path).endswith(\".engine\")\n",
    "    model = YOLO(model_path, task=\"detect\") if is_engine else YOLO(model_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "    assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(str(out_mp4), fourcc, fps_in, (w, h))\n",
    "\n",
    "    times = []; i = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        t0 = time.time()\n",
    "        res = model.predict(source=frame, imgsz=imgsz, conf=conf, device=device, verbose=False)\n",
    "        dt = time.time() - t0\n",
    "        if i >= warmup: times.append(dt)\n",
    "        plotted = res[0].plot()\n",
    "        cv2.putText(plotted, f\"{tag}\", (22, 42), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(plotted, f\"{tag}\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2, cv2.LINE_AA)\n",
    "        writer.write(plotted)\n",
    "        i += 1\n",
    "        if limit and i >= limit: break\n",
    "\n",
    "    cap.release(); writer.release()\n",
    "    mean = float(np.mean(times)) if times else float(\"nan\")\n",
    "    fps  = (1.0/mean) if np.isfinite(mean) else 0.0\n",
    "    return mean, fps, len(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35794,
     "status": "ok",
     "timestamp": 1755257479285,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "jsNLvbwKfu18",
    "outputId": "bacd8de8-512d-409d-eed6-57cac19b01f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /content/outs/yolo11m_int8.engine for TensorRT inference...\n",
      "\n",
      "=== Inference Speed Summary (ignoring first 20 frames) ===\n",
      "Model                 avg ms/frame        FPS   timed frames\n",
      "PyTorch FP32                 27.30      36.63            669\n",
      "TensorRT FP32                30.39      32.91            669\n",
      "TensorRT FP16                17.39      57.50            669\n",
      "TensorRT INT8                14.32      69.84            669\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    results\n",
    "except NameError:\n",
    "    results = [] \n",
    "\n",
    "mean8, fps8, n8 = benchmark_and_write(ENG_INT8, \"TensorRT INT8\", OUT_DIR/\"trt_int8.mp4\")\n",
    "results.append((\"TensorRT INT8\", mean8, fps8, n8))\n",
    "\n",
    "print(\"\\n=== Inference Speed Summary (ignoring first\", WARMUP_FRAMES, \"frames) ===\")\n",
    "print(f\"{'Model':18}  {'avg ms/frame':>14}  {'FPS':>9}  {'timed frames':>13}\")\n",
    "for name, mean, fps, n in results:\n",
    "    ms = mean*1000 if np.isfinite(mean) else float('nan')\n",
    "    print(f\"{name:18}  {ms:14.2f}  {fps:9.2f}  {n:13d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5325,
     "status": "ok",
     "timestamp": 1755257671779,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "xHb86kc1fxOQ",
    "outputId": "55aee13e-5f09-4789-b405-1a67e0374401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Copied 7 files ‚Üí /content/drive/MyDrive/yolo11_trt_outs\n",
      "baseline_pytorch_fp32.mp4         42.9 MB\n",
      "trt_fp16.mp4                      43.2 MB\n",
      "trt_fp32.mp4                      43.1 MB\n",
      "trt_int8.mp4                      46.9 MB\n",
      "yolo11m_fp16.engine               41.9 MB\n",
      "yolo11m_fp32.engine               86.6 MB\n",
      "yolo11m_int8.engine               24.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Mount (safe to re-run)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "SRC = Path(\"/content/outs\")\n",
    "DST = Path(\"/content/drive/MyDrive/yolo11_trt_outs\")\n",
    "DST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy engines and videos\n",
    "copied = 0\n",
    "for p in SRC.glob(\"*\"):\n",
    "    if p.suffix.lower() in {\".engine\", \".mp4\"}:\n",
    "        shutil.copy2(p, DST / p.name)\n",
    "        copied += 1\n",
    "\n",
    "print(f\"Copied {copied} files ‚Üí {DST}\")\n",
    "\n",
    "# List what‚Äôs in Drive with sizes\n",
    "for p in sorted(DST.glob(\"*\")):\n",
    "    sz = os.path.getsize(p) / (1024*1024)\n",
    "    print(f\"{p.name:30}  {sz:6.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3881,
     "status": "ok",
     "timestamp": 1755257701261,
     "user": {
      "displayName": "√ñzg√ºr Cem √ñzgen",
      "userId": "17704929720242838958"
     },
     "user_tz": -180
    },
    "id": "XBX3GRWAgnpY",
    "outputId": "19a42317-35c8-4688-ff00-fa9ca0443b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 480 calibration images to /content/drive/MyDrive/yolo11_trt_outs/calib/images\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "SRC_IMG  = Path(\"/content/calib/images\")\n",
    "DST_CAL  = Path(\"/content/drive/MyDrive/yolo11_trt_outs/calib/images\")\n",
    "DST_CAL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "for p in SRC_IMG.glob(\"*.jpg\"):\n",
    "    shutil.copy2(p, DST_CAL / p.name)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "SRC_YAML = Path(\"/content/calib.yaml\")\n",
    "if SRC_YAML.exists():\n",
    "    shutil.copy2(SRC_YAML, DST_CAL.parent / \"calib.yaml\")\n",
    "\n",
    "print(f\"Copied {count} calibration images to {DST_CAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnLUU2eygvMz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPAb1r/xQNKfwdB3mr7ciH2",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1mTWU--9hHqmto-8FpzH6zqze5I4gG5FE",
     "timestamp": 1755410708127
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
